\section{The Generative Architecture}
The generative architecture is designed to optimize two specific constraints currently encountered by LLM agents. First, we need ways to better individuate agents, by incorporating private data into states, such that outputs directly reflect the perspective of the agent. Failure to successfully accomplish this leads to less pluralistic communities, which can degrade the ultimate quality of the output. Second, we need a way to enable \textit{community evolution}. Ideally, agents who function in a knowledge ecology are able to self-optimize through reproduction with other agents. In other words, evolutionary algorithms are used to merge model weights between agents. The fitness of the agent as understood via evolution is measured against the tasks success.

\subsection{Agent State and its Motivation}

A major breakthrough occurred when OpenAi transitioned from GPT-3 to chat-gpt. The introduction of assistants, the message structure, and the system message as a component applied an advanced model like GPT-3 at the time fundamentally changed the nature of how models were interacting with users. The idea was quite simple, not all information serves the same equal purpose. In a dialog its critical to keep track of who is speaking what, and to include system prompts to help facilitate the role the assistant should be playing in the dialog.

Similarly, we propose rethinking the nature of information being passed in prompting. We hope to fine-tune and in some cases reconstruct a models ability to incoporate beliefs, knowledge, relationships, and memory into its outputs. The strategy is to use Direct Preference Optimization or Odds Ration Preference Optimization to train the model to output different responses based on provided belief or knowledge states. Eventually, the model will begin to form a kind of identity that can be loaded and easily diversified based on the state of the agent as retrieved from the retrieval system. The goal is to compress the information needed to individuate an agent into a representation that can be retrieved and loaded into a model's system state.

\subsection{Evolutionary Model Merging & Mixing}

Recently developers have begun to get access to a wide variety of open-source foundation language models of varying size and trained on differing datasets. Developers were then able to work with these models to do three main things. First, developers were able to fine-tune these models using transfer learning and Lora Adapters. This enhancement enables engineers to make a general purpose model specialize in a particular use case. Moreover, the way in which PEFT models are trained, its possible to take a single general model and build a variet of adapters which can be theoretically loaded on the fly. Secondly, developers began \texit{mixing} models using the Mixture of Experts (MoE) approach. This strategy enabled models to have access to a much larger set of parameters with less compute (VRAM is still a constraint). MoE models uses router layers, to learn which "expert" models to use given a prompt. The router picks a subset of the available models to pass the input through, thereby allowing dynamic inference according to each prompt. Thirdly, A.I. engineers are \textit{merging} models, using tools like mergekit (citation needed). These tools enable engineers to apply different algorithms to combine layers from one or several models together. One can even do a combination of mixing and merging to produce completely unique models without incuing the compute cost of training.
